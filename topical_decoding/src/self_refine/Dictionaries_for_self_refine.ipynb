{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate dictionary for self-refine method "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "1. Imports\n",
    "    - Import libraries\n",
    "    - Import NEWTS dataset\n",
    "    - Import LDA model\n",
    "    \n",
    "2. Create dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of top k words to describe the topic\n",
    "top_k = 20\n",
    "\n",
    "# number of articles in dictionary (1 - 2400)\n",
    "article_num = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Imports\n",
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import gensim\n",
    "from gensim import corpora"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the NEWTS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from NEWTS import read\n",
    "\n",
    "newts_train = read.read_train()\n",
    "newts_test = read.read_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def count_and_store_non_ascii_characters(newts_df):\n",
    "    # Dictionary to store non-ASCII characters and their counts\n",
    "    non_ascii_characters = defaultdict(int)\n",
    "\n",
    "    # Define columns that contain text to be inspected\n",
    "    text_columns = [\"article\", \"summary1\", \"summary2\"]\n",
    "\n",
    "    # Iterate through each row in the DataFrame\n",
    "    for _, row in newts_df.iterrows():\n",
    "        # Iterate through each specified text column\n",
    "        for column in text_columns:\n",
    "            text = row[column]\n",
    "            if isinstance(text, str):  # Ensure the value is a string\n",
    "                # Iterate through each character in the string\n",
    "                for character in text:\n",
    "                    # Check if the character is non-ASCII\n",
    "                    if ord(character) > 127:\n",
    "                        non_ascii_characters[character] += 1\n",
    "\n",
    "    return non_ascii_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character '£' (Unicode: U+00A3) appears 2605 times.\n",
      "Character '‚' (Unicode: U+201A) appears 4387 times.\n",
      "Character 'ö' (Unicode: U+00F6) appears 3988 times.\n",
      "Character 'Ñ' (Unicode: U+00D1) appears 3564 times.\n",
      "Character '∫' (Unicode: U+222B) appears 219 times.\n",
      "Character 'π' (Unicode: U+03C0) appears 215 times.\n",
      "Character '≤' (Unicode: U+2264) appears 2818 times.\n",
      "Character '®' (Unicode: U+00AE) appears 101 times.\n",
      "Character '–' (Unicode: U+2013) appears 1019 times.\n",
      "Character 'à' (Unicode: U+00E0) appears 404 times.\n",
      "Character 'Æ' (Unicode: U+00C6) appears 194 times.\n",
      "Character '¨' (Unicode: U+00A8) appears 371 times.\n",
      "Character 'á' (Unicode: U+00E1) appears 109 times.\n",
      "Character '∏' (Unicode: U+220F) appears 13 times.\n",
      "Character 'â' (Unicode: U+00E2) appears 44 times.\n",
      "Character '±' (Unicode: U+00B1) appears 69 times.\n",
      "Character '¢' (Unicode: U+00A2) appears 108 times.\n",
      "Character 'î' (Unicode: U+00EE) appears 2 times.\n",
      "Character '∞' (Unicode: U+221E) appears 13 times.\n",
      "Character '´' (Unicode: U+00B4) appears 9 times.\n",
      "Character '¥' (Unicode: U+00A5) appears 8 times.\n",
      "Character '∂' (Unicode: U+2202) appears 44 times.\n",
      "Character 'û' (Unicode: U+00FB) appears 75 times.\n",
      "Character 'Œ' (Unicode: U+0152) appears 12 times.\n",
      "Character 'ü' (Unicode: U+00FC) appears 4 times.\n",
      "Character '∆' (Unicode: U+2206) appears 4 times.\n",
      "Character 'í' (Unicode: U+00ED) appears 4 times.\n",
      "Character '§' (Unicode: U+00A7) appears 9 times.\n",
      "Character 'Ç' (Unicode: U+00C7) appears 2 times.\n",
      "Character '•' (Unicode: U+2022) appears 8 times.\n",
      "Character 'ë' (Unicode: U+00EB) appears 17 times.\n",
      "Character 'ò' (Unicode: U+00F2) appears 6 times.\n",
      "Character 'Ö' (Unicode: U+00D6) appears 1 times.\n",
      "Character '¶' (Unicode: U+00B6) appears 4 times.\n",
      "Character 'Ü' (Unicode: U+00DC) appears 3 times.\n",
      "Character 'µ' (Unicode: U+00B5) appears 1 times.\n",
      "Character 'œ' (Unicode: U+0153) appears 1 times.\n",
      "Character 'è' (Unicode: U+00E8) appears 1 times.\n",
      "Character 'ú' (Unicode: U+00FA) appears 1 times.\n"
     ]
    }
   ],
   "source": [
    "# Use the function to count non-ASCII characters in the DataFrame\n",
    "non_ascii_characters_counts = count_and_store_non_ascii_characters(newts_train)\n",
    "\n",
    "# Print the non-ASCII characters and their counts\n",
    "for character, count in non_ascii_characters_counts.items():\n",
    "    print(\n",
    "        f\"Character '{character}' (Unicode: U+{ord(character):04X}) appears {count} times.\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lda_model(model_address: str):\n",
    "    # Loads the LDA model and dictionary from the specified address.\n",
    "    try:\n",
    "        lda = gensim.models.ldamodel.LdaModel.load(\n",
    "            model_address + \"lda.model\", mmap=\"r\"\n",
    "        )\n",
    "        dictionary = corpora.Dictionary.load(model_address + \"dictionary.dic\", mmap=\"r\")\n",
    "        return lda, dictionary\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model or dictionary: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:random_state not set so using default value\n"
     ]
    }
   ],
   "source": [
    "model_address = \"LDA_250/\"\n",
    "lda, dictionary = load_lda_model(model_address)\n",
    "# Warning \"WARNING:root:random_state not set so using default value\" is inconsequential for inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Utility functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_topic_words(lda, topic_id, num_words):\n",
    "    \"\"\"\n",
    "    Returns the top words for a given topic from the LDA model.\n",
    "\n",
    "    :param lda: The LDA model.\n",
    "    :param topic_id: The topic number to get the top words for.\n",
    "    :param num_words: The number of top words to return.\n",
    "    :return: A list of top words for the specified topic.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Get the specified topic. Note: num_words here limits the number of words returned for the topic.\n",
    "        topic_words = lda.show_topic(topic_id, num_words)\n",
    "\n",
    "        # Extract just the words\n",
    "        top_words = [word for word, prob in topic_words]\n",
    "        return top_words\n",
    "    except Exception as e:\n",
    "        print(f\"Error in getting top topic words: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precompute_top_k_words_for_all_topics(lda, top_k=top_k):\n",
    "    \"\"\"\n",
    "    Precompute the top-k words for all topics in the LDA model.\n",
    "\n",
    "    :param lda: The LDA model.\n",
    "    :param top_k: The number of top words to precompute for each topic.\n",
    "    :return: A dictionary mapping topic IDs to their top-k words.\n",
    "    \"\"\"\n",
    "    topic_words = {}\n",
    "    for topic_id in range(lda.num_topics):\n",
    "        topic_words[topic_id] = get_top_topic_words(lda, topic_id, num_words=top_k)\n",
    "    return topic_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precompute top-k words for all topics\n",
    "top_k_words = precompute_top_k_words_for_all_topics(lda, top_k=top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_summary_dicts(newts_train, lda, dictionary, top_k_words, article_num):\n",
    "    \"\"\"\n",
    "    Generate summary dictionaries using precomputed top-k words for each topic.\n",
    "\n",
    "    :param newts_train: The NEWTS training dataset.\n",
    "    :param lda: The LDA model.\n",
    "    :param dictionary: The dictionary of the LDA model (not used in this function but included for consistency).\n",
    "    :param top_k_words: A dictionary mapping topic IDs to their precomputed top-k words.\n",
    "    :return: A list of dictionaries, each containing an article, one of its summaries, and the top-k words for the associated topic.\n",
    "    \"\"\"\n",
    "    summary_dicts = []\n",
    "\n",
    "    # Iterate through article_num articles in the NEWTS training dataset\n",
    "    for _, row in newts_train[:article_num].iterrows():\n",
    "        # Extract article, summaries, and topic ids\n",
    "        article = row[\"article\"]\n",
    "        summary1 = row[\"summary1\"]\n",
    "        summary2 = row[\"summary2\"]\n",
    "        tid1 = row[\"tid1\"]\n",
    "        tid2 = row[\"tid2\"]\n",
    "\n",
    "        # Retrieve precomputed top-k words for each topic id\n",
    "        top_k_words_tid1 = top_k_words[tid1]\n",
    "        top_k_words_tid2 = top_k_words[tid2]\n",
    "\n",
    "        # Create dictionary for tid1 and summary1\n",
    "        dict1 = {\"document\": article, \"summary\": summary1, \"words\": top_k_words_tid1}\n",
    "\n",
    "        # Create dictionary for tid2 and summary2\n",
    "        dict2 = {\"document\": article, \"summary\": summary2, \"words\": top_k_words_tid2}\n",
    "\n",
    "        # Append dictionaries to the list\n",
    "        summary_dicts.append(dict1)\n",
    "        summary_dicts.append(dict2)\n",
    "\n",
    "    return summary_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total entries created: 100\n"
     ]
    }
   ],
   "source": [
    "summary_dicts = generate_summary_dicts(\n",
    "    newts_train, lda, dictionary, top_k_words, article_num\n",
    ")\n",
    "\n",
    "# Validate the length to ensure it matches the expected number of entries\n",
    "print(f\"Total entries created: {len(summary_dicts)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'document': \"The president of the World Bank on Saturday warned the United States was just 'days away' from causing a global economic disaster unless politicians come up with a plan to raise the nation's debt limit and avoid default. 'We're now five days away from a very dangerous moment. I urge US policymakers to quickly come to a resolution before they reach the debt ceiling deadline... Inaction could result in interest rates rising, confidence falling and growth slowing,' World Bank President Jim Yong Kim said in a briefing following a meeting of the bank's Development Committee. 'If this comes to pass, it could be a disastrous event for the developing world, and that will in turn greatly hurt developed economies as well,' he said. Scroll down for video . Disastrous: World Bank chief Jim Yong Kim warned Saturday that the United States was headed toward peril as politicians failed again to resolve a standoff over the budget . The alarming remarks from the Korean-American came after three days of talks revolving around meetings of the 188-nation International Monetary Fund and its sister lending agency, the World Bank, where top officials pressed the US to resolve the political impasse over the debt ceiling. The standoff has blocked approval of legislation to increase the government's borrowing limit before a fast-approaching Thursday deadline. US Treasury Secretary Jacob Lew has warned that he will exhaust his borrowing authority on Thursday and the government will face the prospect of defaulting on its debt unless Congress raises the $16.7 trillion borrowing limit. 'We know there are problems,' Tharman Shanmugaratnam, the head of the IMF's policy-steering committee and Singapore's finance minister, told a news conference at the end of the IMF meeting. 'We know there are near-term risks, the most obvious one being what's going on in the US with regard to the fiscal deficit.' But one of the big near-term concerns, the expectation that the US Federal Reserve will start scaling back its massive stimulus program for the economy, is actually pointing to a positive development, Tharman said. Talks: Jim Yong Kim, (left), president, World Bank Group, and Christine Lagarde, International Monetary Fund (IMF) Managing Director, talk before a meeting of the Development Committee during the World Bank/IMF Annual Meetings in Washington . It means that the U.S. economy is strong enough to withstand a reduction of the stimulus. IMF officials have been forecasting that the strengthening U.S. economy will be a main driver of the global economy in the coming year. At the same time, developing country economies are slowing and their markets have been unsettled since May by anticipation that the Fed will soon begin tapering its $85-billion-a-month bond purchases, which poured cash into the economy to stimulate growth. 'The eventual normalisation of monetary policy as economies recover in the West will be a net positive for the emerging economies,' Tharman said, meaning that the strength of the major economies would help carry the global economy forward. Lew told finance ministers that the US understood the role it played as 'the anchor of the international financial system', and assured them the administration was doing all it could to reach a resolution on the debt. Stalemate: Speaker of the House John Boehner walks to his office after a meeting with fellow Republicans at the Capitol in Washington on Saturday . An effort Saturday to pass a one-year extension of the borrowing limit failed to get sufficient votes. But in a more hopeful sign, negotiations began between Democratic and Republican Senate leaders to end the impasse. Mario Draghi, head of the European Central Bank, was one of a number of officials who were guardedly confident that an eleventh-hour deal would be reached, as it has in similar standoffs in the past. 'I still think it is unthinkable that an agreement won't be found,' Draghi told reporters on Saturday. 'If this situation were to last a long time, it would be very negative for the U.S. economy and the world economy and could certainly harm the recovery.' But once Draghi moved beyond the US impasse, he sounded upbeat about the prospects for a European recovery. That in itself is a dramatic turn from the past three years, when global financial leaders were taken up with waves of crisis sweeping across the region and necessitating a series of international rescue loans. Business not as usual: President Barack Obama yesterday discussed challenges from the partial government shutdown small business owners . The IMF called on emerging economies, which have been the drivers of the global economy in recent years, to undertake reforms that will help their economies better withstand the scaling back of monetary stimulus in the U.S. and other major economies. When the stimulus money was flowing, emerging economies benefited from investments as investors were attracted by their relatively higher rates of interest vis-a-vis the United States and other major economies. But many of those same countries that benefited from capital flows have been rocked since May, as the investment flows reversed and flowed back toward the US as rates here began to rise. Alexandre Tombini, the head Brazil's central bank, told the IMF steering committee that the worries about the US and global economies might be overblown. 'A while ago there was an excess of exuberance and now perhaps an excess of pessimism,' he said. Negotiating: Senate Majority Leader Harry Reid of Nevada is surrounded by reporters as he leaves the Senate floor to meet with Senate Democrats regarding the government shutdown and debt ceiling . Time is ticking: Republican Senator from Kentucky Mitch McConnell walks off the Senate floor after a vote on the motion to proceed on the Debt Limit Bill . The stark warnings from the World Bank came the same day that House Speaker John Boehner today told fellow Republicans that his talks with President Barack Obama have stalled. 'The Senate needs to hold tough,' Representative Greg Walden said Boehner told House GOP lawmakers. 'The president now isn't negotiating with us.' Obama rejected the speaker's effort to lift the debt ceiling for six weeks and reopen government in exchange for a budget negotiating process. Attention now turns to the Senate, where a bipartisan group of Senators are working on a separate plan to reopen the government. New plan: Republican Senator from Maine Susan Collins leaves a Republican meeting and heads to the Senate floor to vote on the motion to proceed on the Debt Limit Bill . Word of the negotiations between Senate Majority Leader Harry Reid, and the top Republican, Senator Mitch McConnell of Kentucky, emerged as the Senate, as expected, rejected a Democratic effort to raise the government's borrowing limit through next year. Republicans objected because they want the extension to be accompanied by spending cuts. The calendar is edging closer to the October 17 deadline to raise the federal debt ceiling. After that, administration officials have said the government will deplete its ability to borrow money, risking a first-time federal default that could jolt the world economy.\", 'summary': 'The leader of the World Bank urged the US to take action before the borrowing deadline. The US Congress needed to come to an agreement to raise the borrowing limit, as the UD treasury secretary had stated his authority had reached its limits in the matter. Republicans shot down the Democratic proposal to increase the borrowing limit, putting a federal default at risk that would affect the global economy.', 'words': ['house', 'committee', 'congress', 'senate', 'republican', 'republicans', 'senator', 'rep', 'federal', 'democrats', 'sen', 'reid', 'chamber', 'democratic', 'capitol', 'government', 'congressional', 'lawmakers', 'gop', 'democrat']}\n"
     ]
    }
   ],
   "source": [
    "print(summary_dicts[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "\n",
    "def count_and_store_non_ascii_characters(dict_list):\n",
    "    # Dictionary to store non-ASCII characters and their counts\n",
    "    non_ascii_characters = defaultdict(int)\n",
    "\n",
    "    # Iterate through each dictionary in the list\n",
    "    for dictionary in dict_list:\n",
    "        # Iterate through all string values in the dictionary\n",
    "        for text in dictionary.values():\n",
    "            if isinstance(text, str):  # Ensure the value is a string\n",
    "                # Iterate through each character in the string\n",
    "                for character in text:\n",
    "                    # Check if the character is non-ASCII\n",
    "                    if ord(character) > 127:\n",
    "                        non_ascii_characters[character] += 1\n",
    "\n",
    "    return non_ascii_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total count of non-ASCII characters: 0\n"
     ]
    }
   ],
   "source": [
    "# Count non-ASCII characters in the dictionary and store them\n",
    "non_ascii_characters = count_and_store_non_ascii_characters(summary_dicts)\n",
    "\n",
    "# Output the characters and their counts\n",
    "for character, count in non_ascii_characters.items():\n",
    "    print(\n",
    "        f\"Character '{character}' (Unicode: U+{ord(character):04X}) appears {count} times.\"\n",
    "    )\n",
    "\n",
    "# If you want to know the total count of non-ASCII characters\n",
    "total_non_ascii_characters = sum(non_ascii_characters.values())\n",
    "print(f\"Total count of non-ASCII characters: {total_non_ascii_characters}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_ascii_characters(list_of_dicts):\n",
    "    for dictionary in list_of_dicts:\n",
    "        for key, value in dictionary.items():\n",
    "            if isinstance(value, str):\n",
    "                # Remove non-ASCII characters using a comprehension\n",
    "                dictionary[key] = \"\".join(char for char in value if ord(char) <= 127)\n",
    "    return list_of_dicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_dicts = remove_non_ascii_characters(summary_dicts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store dictionary to json file\n",
    "import json\n",
    "\n",
    "with open(\"summary_dicts.json\", \"w\") as f:\n",
    "    json.dump(summary_dicts, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate sumamries for Balint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "from transformers import BartTokenizer, BartForConditionalGeneration\n",
    "\n",
    "# Load the BART tokenizer and model\n",
    "tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n",
    "\n",
    "\n",
    "def remove_non_ascii(text):\n",
    "    return \"\".join(char for char in text if ord(char) < 128)\n",
    "\n",
    "\n",
    "def generate_and_store_summaries_with_articles(newts_train, k, tokenizer, model):\n",
    "    # List to store dictionaries containing the article, two original summaries, and the generated summary\n",
    "    data_to_store = []\n",
    "\n",
    "    for i in range(k):\n",
    "        article_text = newts_train.iloc[i][\"article\"]\n",
    "        summary1 = newts_train.iloc[i][\"summary1\"]\n",
    "        summary2 = newts_train.iloc[i][\"summary2\"]\n",
    "\n",
    "        # Remove non-ASCII characters\n",
    "        cleaned_article_text = remove_non_ascii(article_text)\n",
    "        cleaned_summary1 = remove_non_ascii(summary1)\n",
    "        cleaned_summary2 = remove_non_ascii(summary2)\n",
    "\n",
    "        # Encode article\n",
    "        input_ids = tokenizer(\n",
    "            cleaned_article_text, return_tensors=\"pt\", truncation=True, max_length=1024\n",
    "        ).input_ids\n",
    "\n",
    "        # Generate Summary Text Ids\n",
    "        summary_text_ids = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            bos_token_id=model.config.bos_token_id,\n",
    "            eos_token_id=model.config.eos_token_id,\n",
    "            length_penalty=2.0,\n",
    "            max_length=142,\n",
    "            min_length=56,\n",
    "            num_beams=4,\n",
    "            do_sample=True,\n",
    "            top_k=k,\n",
    "        )\n",
    "\n",
    "        # Decode generated summary and remove non-ASCII characters\n",
    "        generated_summary = remove_non_ascii(\n",
    "            tokenizer.decode(summary_text_ids[0], skip_special_tokens=True)\n",
    "        )\n",
    "\n",
    "        # Append to the list\n",
    "        data_to_store.append(\n",
    "            {\n",
    "                \"article\": cleaned_article_text,\n",
    "                \"summary1\": cleaned_summary1,\n",
    "                \"summary2\": cleaned_summary2,\n",
    "                \"generated_summary\": generated_summary,\n",
    "            }\n",
    "        )\n",
    "\n",
    "    # Store in a json file\n",
    "    with open(\"summaries_with_articles.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data_to_store, f, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming newts_train is your DataFrame, tokenizer and model are loaded and configured\n",
    "generate_and_store_summaries_with_articles(\n",
    "    newts_train, k=50, tokenizer=bart_base_tokenizer, model=bart_base_model\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_proj_basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

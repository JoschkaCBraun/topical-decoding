{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the generate function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from topical_decoding.utils.newts_utils import read_train, read_test\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the model and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "T5_base_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "T5_base_model = T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the NEWTS dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the NEWTS training set\n",
    "newts_test = read_test(\"../../NEWTS/NEWTS_test_600.csv\")\n",
    "newts_train = read_train(\"../../NEWTS/NEWTS_train_2400.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AssignmentId</th>\n",
       "      <th>docId</th>\n",
       "      <th>article</th>\n",
       "      <th>tid1</th>\n",
       "      <th>tid2</th>\n",
       "      <th>words1</th>\n",
       "      <th>words2</th>\n",
       "      <th>phrases1</th>\n",
       "      <th>phrases2</th>\n",
       "      <th>sentences1</th>\n",
       "      <th>sentences2</th>\n",
       "      <th>summary1</th>\n",
       "      <th>summary2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3EG49X351WE8VLP4S0TIYZF3V476X2</td>\n",
       "      <td>094372190d52acbce61a73ec16b2217d1a60276f</td>\n",
       "      <td>The president of the World Bank on Saturday wa...</td>\n",
       "      <td>175</td>\n",
       "      <td>110</td>\n",
       "      <td>house, committee, congress, senate, republican...</td>\n",
       "      <td>billion, figures, economy, global, growth, eco...</td>\n",
       "      <td>senate and congress, congressional pressure, y...</td>\n",
       "      <td>economic growth, global growth, billion dollar...</td>\n",
       "      <td>This topic is about the senate and congress, c...</td>\n",
       "      <td>This topic is about economic growth involving ...</td>\n",
       "      <td>The leader of the World Bank urged the US to t...</td>\n",
       "      <td>The US economy will be a driving factor in the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3DOCMVPBTPGBQCHSPBSQ28AROFXNNI</td>\n",
       "      <td>bc733fb96fd73496e10fcff3c640ee11c4df3d7a</td>\n",
       "      <td>By . Nick Harris . Manchester City are the bes...</td>\n",
       "      <td>152</td>\n",
       "      <td>217</td>\n",
       "      <td>united, manchester, liverpool, chelsea, league...</td>\n",
       "      <td>club, team, season, players, england, football...</td>\n",
       "      <td>Manchester United's manager, Premier League, t...</td>\n",
       "      <td>football league, the team's fans, football pla...</td>\n",
       "      <td>This topic is about Manchester United's manage...</td>\n",
       "      <td>This topic is about a football league having a...</td>\n",
       "      <td>Premier league is the most paying football lea...</td>\n",
       "      <td>Manchester city players earn the largest amoun...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     AssignmentId                                     docId  \\\n",
       "0  3EG49X351WE8VLP4S0TIYZF3V476X2  094372190d52acbce61a73ec16b2217d1a60276f   \n",
       "1  3DOCMVPBTPGBQCHSPBSQ28AROFXNNI  bc733fb96fd73496e10fcff3c640ee11c4df3d7a   \n",
       "\n",
       "                                             article  tid1  tid2  \\\n",
       "0  The president of the World Bank on Saturday wa...   175   110   \n",
       "1  By . Nick Harris . Manchester City are the bes...   152   217   \n",
       "\n",
       "                                              words1  \\\n",
       "0  house, committee, congress, senate, republican...   \n",
       "1  united, manchester, liverpool, chelsea, league...   \n",
       "\n",
       "                                              words2  \\\n",
       "0  billion, figures, economy, global, growth, eco...   \n",
       "1  club, team, season, players, england, football...   \n",
       "\n",
       "                                            phrases1  \\\n",
       "0  senate and congress, congressional pressure, y...   \n",
       "1  Manchester United's manager, Premier League, t...   \n",
       "\n",
       "                                            phrases2  \\\n",
       "0  economic growth, global growth, billion dollar...   \n",
       "1  football league, the team's fans, football pla...   \n",
       "\n",
       "                                          sentences1  \\\n",
       "0  This topic is about the senate and congress, c...   \n",
       "1  This topic is about Manchester United's manage...   \n",
       "\n",
       "                                          sentences2  \\\n",
       "0  This topic is about economic growth involving ...   \n",
       "1  This topic is about a football league having a...   \n",
       "\n",
       "                                            summary1  \\\n",
       "0  The leader of the World Bank urged the US to t...   \n",
       "1  Premier league is the most paying football lea...   \n",
       "\n",
       "                                            summary2  \n",
       "0  The US economy will be a driving factor in the...  \n",
       "1  Manchester city players earn the largest amoun...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newts_train.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2400, 13)\n"
     ]
    }
   ],
   "source": [
    "print(newts_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A charge of making a false bomb threat has been dropped against a man who carried a backpack containing a rice cooker near a crowd marking the first anniversary of the Boston Marathon bombings in April, prosecutors said Wednesday. Investigators dropped the charge because they say the suspect, Kevin Edson, 25, did not communicate an \"overt threat that an incendiary device would be detonated,\" Jake Wark, a spokesman for the Suffolk County district attorney, told CNN. Edson was arrested after carrying the backpack with a rice cooker near the finish line on Boylston Street in Boston while survivors of the 2013 bombing were commemorating its anniversary on April 15. In the 2013 attack, two pressure-cooker bombs exploded, killing three people and wounding at least 264 others. A barefoot Edson, carrying a backpack and wearing black clothes with a veil and hat covering his face, screamed and yelled near the end of the anniversary event on Boylston Street, drawing officers' attention, police said. Wark said prosecutors decided to drop the charge after reviewing statements Edson made at the scene as well as video shot by onlookers. Edson still faces charges of disturbing the peace, disorderly conduct, disturbing a public assembly and possession or use of a hoax device. His next court date is scheduled for August 27. The charge of making a false bomb threat carried the most severe sentence upon conviction -- up to 20 years in prison. Possession or use of a hoax device carries a maximum sentence of five years, Wark said. A judge in April set bail at $100,000 for Edson. The judge also ordered Edson, also known as Kayvon Edson, to be seen at a state hospital that evaluates defendants' sanity after a mental health professional told the judge that Edson has a history of psychiatric disturbances. Defense attorney Shannon Lopez did not immediately return calls seeking comment. How can you keep 26.2 miles safe?\n"
     ]
    }
   ],
   "source": [
    "# Select an example article by its index.\n",
    "example_article = newts_train.iloc[23]\n",
    "\n",
    "# Print the article\n",
    "print(example_article[\"article\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate a summary for articles in the NEWTS training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (829 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "task_prefix = \"summarize: \"\n",
    "# flan_prompt_prefix = \"Briefly summarize this paragraph: \"\n",
    "\n",
    "min_idx = 0\n",
    "max_idx = 3\n",
    "sentences = newts_test[\"article\"][min_idx:max_idx].tolist()\n",
    "inputs = T5_base_tokenizer(\n",
    "    [task_prefix + sentence for sentence in sentences],\n",
    "    return_tensors=\"pt\",\n",
    "    padding=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal setup using generate function from transformers library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_sequences = T5_base_model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    do_sample=False,  # disable sampling to test if batching affects output\n",
    "    max_length=100,\n",
    "    min_length=30,\n",
    "    # early_stopping=False,\n",
    "    num_beams=1,\n",
    "    # no_repeat_ngram_size=3,\n",
    "    num_return_sequences=1,\n",
    "    # top_k=0,\n",
    "    # bos_token_id=model.config.bos_token_id,\n",
    "    # eos_token_id=model.config.eos_token_id,\n",
    "    # length_penalty=2.0,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input: An American tourist has spent the night stranded in the Blue Mountains, west of Sydney, after she fell 15 metres off a cliff while bushwalking. The 25-year-old from the US state of Wisconsin was walking near Pulpit Rock, Mount Victoria with a group of friends on Friday when she slipped from a track. She fell about 15 metres and rolled a further 20 metres down a steep slope, police say. Rescue teams escort a 25-year-old US tourist after she spent the night stranded in the Blue Mountains after falling 15 metres off a cliff . Rescue crews found the woman suffering a possible broken ankle and broken ribs. She remained with an ambulance team overnight due to low light and foggy weather conditions. Blue Mountains Police Rescue Sergeant Dallas Atkinson told ABC a helicopter was deployed to finish the rescue this morning. 'After she fell yesterday she was accessed a short time later by police and ambulance,' Sergeant Atkinson said. 'It was determined that she had sustained leg and chest injuries and a rescue plan was put in place where the patient was going to be carried out. Rescue crews found the Wisconsin woman had suffered a possible broken ankle and broken ribs . Rescue teams had to wait for the heavy fog to lift so they could winch the woman out via a helicopter . 'But due to the terrain in the prevailing weather that plan was aborted.' Rescue teams had to wait for the fog to lift so they could winch the woman out via a helicopter.\n",
      "Generated summary 1: The 25-year-old from Wisconsin was walking near Pulpit Rock, Mount Victoria with a group of friends on Friday. She fell about 15 metres and rolled a further 20 metres down a steep slope, police say. She remained with an ambulance team overnight due to low light and foggy weather conditions.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input: A 41-year-old British man has been charged after he allegedly relieved himself in an ice machine at an Orlando hotel. Andrew Stewart Wood, of Havant, Hampshire, was accused of urinating into the ice machine at the Hard Rock Hotel in the Universal theme park resort in the early hours of Tuesday. A guest reportedly told a security guard at the luxury hotel that there was a very intoxicated man on the premises. Andrew Wood, 41, from the UK, was arrested at Orlando's Hard Rock Hotel after being caught urinating in an ice machine while intoxicated . The guard located Wood and saw him urinating into an ice machine. When he tried to stop him, Wood became belligerent, began shouting and would not cooperate. According to the police report, the security guard tried to have the guest identify himself and tell him what room he was staying in but he continued disturbing the peace and quiet of other guests. The security guard then called the police. Police removed Wood from the premises after he refused to stay in his room. Officers said that Wood was extremely intoxicated 'with the odor of alcohol emanating from his person and having random outbursts'. He was arrested on disorderly conduct charges and placed in jail where he continued to act out. Wood posted $1,000 bond and was released from Orange County Jail on Tuesday afternoon, according to a booking report. In a statement to Daily Mail Online, Hard Rock Hotel at Universal Orlando said: 'The health and safety of our guests is our top priority and we have removed the ice machine from service.' The hotel was unable to confirm if Wood was still a guest, adding: 'For security reasons, we are unable to provide guest information.' A standard room at the hotel costs around $294 per night. The luxury Hard Rock hotel in Florida is attached to the Universal Studios theme park - a standard room starts at around $300 a night .\n",
      "Generated summary 2: Andrew Wood, 41, from Hampshire, urinated in ice machine at Hard Rock Hotel. A guest told a security guard that there was a very intoxicated man on the premises. Wood was arrested on disorderly conduct charges and placed in jail.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input: Tehran, Iran (CNN)The Basij is a militia made up of fighters loyal to Iran's religious leaders; their mission is to protect the country's Islamic order. To do that, they will go to any lengths necessary, including -- they say -- taking on ISIS. \"We all are prepared to go and destroy ISIS totally,\" one Basij commander told CNN. \"If our Imam, our Supreme Leader orders us, we will destroy ISIS.\" The commander says that, so far, the Basij has not been caught up in the fight against the feared Islamic extremists currently waging war in parts of Iraq and Syria. But Iran's elite Revolutionary Guard's Quds Force, led by General Qassem Suleimani, is already training, advising and supporting Iraqi Shia militias in their fight against ISIS. Suleimani was accused of involvement in the Shia insurgency against U.S. forces during the Iraq war. Today he is a celebrity to many Iraqis and Iranians. That is symbolic of the gulf that still exists between Iran and the U.S., regardless of any thaw in relations in the wake of the recent agreement on a framework nuclear deal and ongoing talks. Iran, militias' involvement in ISIS fight a mixed blessing . Iranian officials, who believe their strategy is making a difference in the fight against ISIS, say they would like better cooperation with the U.S., but point out that the level of trust simply isn't there. \"At the moment, we consider the United States to be a threat to us because its policies and actions are threatening to us,\" said General Ahmad Reza Pourdastan, commander of Iran's ground forces. \"We would like the US to change its rhetoric and tone of voice so that our nation could have more trust in U.S. military leadership.\" And the feeling is mutual: the U.S., which is leading the air campaign against ISIS in Iraq, has denied any direct coordination with Iran. Iran will do what it takes to fight ISIS . Iranians believe air strikes against ISIS are not effective, and feel that the U.S. and its allies are not trying seriously enough to defeat the group. Iran's President Hassan Rouhani told CNN's Christiane Amanpour last September that \"the aerial bombardment campaign is mostly ... a form of theater, rather than a serious battle against terrorism.\" \"The battle in Iraq is very important to Iran,\" explained Mohammed Marandi, a professor at Tehran University. \"The Iranians believe that the Americans, if they wanted to, could do a lot more to put pressure on their allies. And also, if they were serious about air strikes, they could do a lot more.\" It's a point the U.S., of course, disagrees with -- U.S. President Barack Obama has vowed to \"degrade and ultimately defeat\" the terror group. But Iran remains unconvinced. \"If they want to destroy ISIS, it is possible for them to achieve that,\" said Major-General Hassan Firouzabadi, Iran's chief of general staff. \"The U.S. military and intelligence organizations have many ways to strike at ISIS, but we have not seen anything so far except intelligence gathering from the U.S. and Britain,\" he said. \"We hope that one day, because of their national interests and the will of their nations, the U.S. and the UK will decide to really fight ISIS.\" In ISIS, Iran and the U.S. share a common enemy, but -- for now at least -- no apparent common strategy.Will Iran-Saudi proxy war erupt?\n",
      "Generated summary 3: Iran and the U.S. share a common enemy, but for now at least -- no apparent common strategy. Iran and the U.S. share a common enemy, but for now at least -- no apparent common strategy.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Output:\\n\" + 100 * \"-\")\n",
    "for idx, output_sequence in enumerate(output_sequences):\n",
    "    output = T5_base_tokenizer.decode(output_sequence, skip_special_tokens=True)\n",
    "    print(\"Input: {}\".format(sentences[idx]))\n",
    "    print(\"Generated summary {}: {}\".format(idx + 1, output))\n",
    "    print(100 * \"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seperate generate function via Subclassing and Overriding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomT5Model(T5ForConditionalGeneration):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # Initialize additional attributes if necessary\n",
    "\n",
    "    def generate(self, input_ids, attention_mask=None, **kwargs):\n",
    "        # Dummy implementation: Call the original generate method\n",
    "        output_sequences = super().generate(\n",
    "            input_ids=input_ids, attention_mask=attention_mask, **kwargs\n",
    "        )\n",
    "\n",
    "        # Dummy token reweighting (multiplying by 1, which does nothing)\n",
    "        # This is where you'd integrate the LDA model logic in the future\n",
    "\n",
    "        return output_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example Usage\n",
    "custom_model = CustomT5Model.from_pretrained(\"google/flan-t5-base\")\n",
    "\n",
    "# Assuming 'inputs' is a dictionary with 'input_ids' and 'attention_mask'\n",
    "output_sequences = custom_model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    max_length=100,\n",
    "    min_length=30,\n",
    "    num_beams=1,\n",
    "    num_return_sequences=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output from custom model:\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input: An American tourist has spent the night stranded in the Blue Mountains, west of Sydney, after she fell 15 metres off a cliff while bushwalking. The 25-year-old from the US state of Wisconsin was walking near Pulpit Rock, Mount Victoria with a group of friends on Friday when she slipped from a track. She fell about 15 metres and rolled a further 20 metres down a steep slope, police say. Rescue teams escort a 25-year-old US tourist after she spent the night stranded in the Blue Mountains after falling 15 metres off a cliff . Rescue crews found the woman suffering a possible broken ankle and broken ribs. She remained with an ambulance team overnight due to low light and foggy weather conditions. Blue Mountains Police Rescue Sergeant Dallas Atkinson told ABC a helicopter was deployed to finish the rescue this morning. 'After she fell yesterday she was accessed a short time later by police and ambulance,' Sergeant Atkinson said. 'It was determined that she had sustained leg and chest injuries and a rescue plan was put in place where the patient was going to be carried out. Rescue crews found the Wisconsin woman had suffered a possible broken ankle and broken ribs . Rescue teams had to wait for the heavy fog to lift so they could winch the woman out via a helicopter . 'But due to the terrain in the prevailing weather that plan was aborted.' Rescue teams had to wait for the fog to lift so they could winch the woman out via a helicopter.\n",
      "Generated summary 1: The 25-year-old from Wisconsin was walking near Pulpit Rock, Mount Victoria with a group of friends on Friday. She fell about 15 metres and rolled a further 20 metres down a steep slope, police say. She remained with an ambulance team overnight due to low light and foggy weather conditions.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input: A 41-year-old British man has been charged after he allegedly relieved himself in an ice machine at an Orlando hotel. Andrew Stewart Wood, of Havant, Hampshire, was accused of urinating into the ice machine at the Hard Rock Hotel in the Universal theme park resort in the early hours of Tuesday. A guest reportedly told a security guard at the luxury hotel that there was a very intoxicated man on the premises. Andrew Wood, 41, from the UK, was arrested at Orlando's Hard Rock Hotel after being caught urinating in an ice machine while intoxicated . The guard located Wood and saw him urinating into an ice machine. When he tried to stop him, Wood became belligerent, began shouting and would not cooperate. According to the police report, the security guard tried to have the guest identify himself and tell him what room he was staying in but he continued disturbing the peace and quiet of other guests. The security guard then called the police. Police removed Wood from the premises after he refused to stay in his room. Officers said that Wood was extremely intoxicated 'with the odor of alcohol emanating from his person and having random outbursts'. He was arrested on disorderly conduct charges and placed in jail where he continued to act out. Wood posted $1,000 bond and was released from Orange County Jail on Tuesday afternoon, according to a booking report. In a statement to Daily Mail Online, Hard Rock Hotel at Universal Orlando said: 'The health and safety of our guests is our top priority and we have removed the ice machine from service.' The hotel was unable to confirm if Wood was still a guest, adding: 'For security reasons, we are unable to provide guest information.' A standard room at the hotel costs around $294 per night. The luxury Hard Rock hotel in Florida is attached to the Universal Studios theme park - a standard room starts at around $300 a night .\n",
      "Generated summary 2: Andrew Wood, 41, from Hampshire, urinated in ice machine at Hard Rock Hotel. A guest told a security guard that there was a very intoxicated man on the premises. Wood was arrested on disorderly conduct charges and placed in jail.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Input: Tehran, Iran (CNN)The Basij is a militia made up of fighters loyal to Iran's religious leaders; their mission is to protect the country's Islamic order. To do that, they will go to any lengths necessary, including -- they say -- taking on ISIS. \"We all are prepared to go and destroy ISIS totally,\" one Basij commander told CNN. \"If our Imam, our Supreme Leader orders us, we will destroy ISIS.\" The commander says that, so far, the Basij has not been caught up in the fight against the feared Islamic extremists currently waging war in parts of Iraq and Syria. But Iran's elite Revolutionary Guard's Quds Force, led by General Qassem Suleimani, is already training, advising and supporting Iraqi Shia militias in their fight against ISIS. Suleimani was accused of involvement in the Shia insurgency against U.S. forces during the Iraq war. Today he is a celebrity to many Iraqis and Iranians. That is symbolic of the gulf that still exists between Iran and the U.S., regardless of any thaw in relations in the wake of the recent agreement on a framework nuclear deal and ongoing talks. Iran, militias' involvement in ISIS fight a mixed blessing . Iranian officials, who believe their strategy is making a difference in the fight against ISIS, say they would like better cooperation with the U.S., but point out that the level of trust simply isn't there. \"At the moment, we consider the United States to be a threat to us because its policies and actions are threatening to us,\" said General Ahmad Reza Pourdastan, commander of Iran's ground forces. \"We would like the US to change its rhetoric and tone of voice so that our nation could have more trust in U.S. military leadership.\" And the feeling is mutual: the U.S., which is leading the air campaign against ISIS in Iraq, has denied any direct coordination with Iran. Iran will do what it takes to fight ISIS . Iranians believe air strikes against ISIS are not effective, and feel that the U.S. and its allies are not trying seriously enough to defeat the group. Iran's President Hassan Rouhani told CNN's Christiane Amanpour last September that \"the aerial bombardment campaign is mostly ... a form of theater, rather than a serious battle against terrorism.\" \"The battle in Iraq is very important to Iran,\" explained Mohammed Marandi, a professor at Tehran University. \"The Iranians believe that the Americans, if they wanted to, could do a lot more to put pressure on their allies. And also, if they were serious about air strikes, they could do a lot more.\" It's a point the U.S., of course, disagrees with -- U.S. President Barack Obama has vowed to \"degrade and ultimately defeat\" the terror group. But Iran remains unconvinced. \"If they want to destroy ISIS, it is possible for them to achieve that,\" said Major-General Hassan Firouzabadi, Iran's chief of general staff. \"The U.S. military and intelligence organizations have many ways to strike at ISIS, but we have not seen anything so far except intelligence gathering from the U.S. and Britain,\" he said. \"We hope that one day, because of their national interests and the will of their nations, the U.S. and the UK will decide to really fight ISIS.\" In ISIS, Iran and the U.S. share a common enemy, but -- for now at least -- no apparent common strategy.Will Iran-Saudi proxy war erupt?\n",
      "Generated summary 3: Iran and the U.S. share a common enemy, but for now at least -- no apparent common strategy. Iran and the U.S. share a common enemy, but for now at least -- no apparent common strategy.\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Output from custom model:\\n\" + 100 * \"-\")\n",
    "for idx, output_sequence in enumerate(output_sequences):\n",
    "    output = T5_base_tokenizer.decode(output_sequence, skip_special_tokens=True)\n",
    "    print(\"Input: {}\".format(sentences[idx]))\n",
    "    print(\"Generated summary {}: {}\".format(idx + 1, output))\n",
    "    print(100 * \"-\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works perfectly fine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Alter generate function and take topic into account during generation\n",
    "#### Option 1: \n",
    "discard/block words based on their topic affiliation (expect this to be bad)\n",
    "cannot write coherent/grammatical sentences if blocking too many words\n",
    "block words from second topic\n",
    "topics could overlap (could exclude the overlapping)\n",
    "binary mask over the vocabulary to multiply probability vector with\n",
    "\n",
    "#### Option 2:\n",
    "temperature scaling if needed (to make the factor important)\n",
    "multiply tokens that belong to the target topic with a factor > 1 \n",
    "rescale/normalize  after multiplying\n",
    "Alternative\n",
    "select top k tokens and then over-sample tokens from the topic. \n",
    "Start out with uniform distribution over top-k tokens. Multiply each token belonging to the target topic with factor (e.g. 2-5) and then rescale entire probability distribution accordingly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_lda_model(model_address: str):\n",
    "    # Loads the LDA model and dictionary from the specified address.\n",
    "    try:\n",
    "        lda = gensim.models.ldamodel.LdaModel.load(\n",
    "            model_address + \"lda.model\", mmap=\"r\"\n",
    "        )\n",
    "        dictionary = corpora.Dictionary.load(model_address + \"dictionary.dic\", mmap=\"r\")\n",
    "        return lda, dictionary\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model or dictionary: {e}\")\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:random_state not set so using default value\n"
     ]
    }
   ],
   "source": [
    "model_address = \"../../LDA_250/\"\n",
    "lda, dictionary = load_lda_model(model_address)\n",
    "# Warning \"WARNING:root:random_state not set so using default value\" is inconsequential\n",
    "# for inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_topic_words(lda, topic_number, num_words=100):\n",
    "    \"\"\"\n",
    "    Returns the top words for a given topic from the LDA model.\n",
    "\n",
    "    :param lda: The LDA model.\n",
    "    :param topic_number: The topic number to get the top words for.\n",
    "    :param num_words: The number of top words to return.\n",
    "    :return: A list of top words for the specified topic.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        lda_topics = lda.show_topics(formatted=True)\n",
    "        _, topic_words = lda_topics[topic_number]\n",
    "        words_with_probs = topic_words.split(\" + \")\n",
    "\n",
    "        # Extracting just the words\n",
    "        top_words = [\n",
    "            word_prob.split(\"*\")[1].strip()\n",
    "            for word_prob in words_with_probs[:num_words]\n",
    "        ]\n",
    "        return top_words\n",
    "    except Exception as e:\n",
    "        print(f\"Error in getting top topic words: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomT5Model(T5ForConditionalGeneration):\n",
    "    def __init__(self, tokenizer, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def adjust_logits_during_generation(\n",
    "        self, logits, block_token_ids, cur_len, max_length\n",
    "    ):\n",
    "        # Modify this method to adjust logits according to your needs\n",
    "        if cur_len < max_length:\n",
    "            for token_id in block_token_ids:\n",
    "                logits[:, token_id] = -float(\"inf\")\n",
    "        return logits\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask=None,\n",
    "        block_topic=-1,\n",
    "        lda_model=None,\n",
    "        dictionary=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # Checking if a topic should be blocked\n",
    "        block_token_ids = []\n",
    "        if block_topic >= 0 and lda_model is not None and dictionary is not None:\n",
    "            top_words = get_top_topic_words(lda_model, block_topic)\n",
    "            block_token_ids = [\n",
    "                self.tokenizer.encode(word, add_special_tokens=False)[0]\n",
    "                for word in top_words\n",
    "            ]\n",
    "\n",
    "            # Modify logits_processor to block the selected words\n",
    "            logits_processor = transformers.generation_logits_process.LogitsProcessorList(\n",
    "                [\n",
    "                    transformers.generation_logits_process.HammingDiversityLogitsProcessor(\n",
    "                        group_size=1\n",
    "                    ),\n",
    "                    transformers.generation_logits_process.InfNanRemoveLogitsProcessor(),\n",
    "                    transformers.generation_logits_process.RepetitionPenaltyLogitsProcessor(\n",
    "                        penalty=1.2\n",
    "                    ),\n",
    "                    transformers.generation_logits_process.ForbiddenTokensLogitsProcessor(\n",
    "                        forbidden_tokens=block_token_ids\n",
    "                    ),\n",
    "                ]\n",
    "            )\n",
    "            kwargs[\"logits_processor\"] = logits_processor\n",
    "\n",
    "        output_sequences = super().generate(\n",
    "            input_ids=input_ids, attention_mask=attention_mask, **kwargs\n",
    "        )\n",
    "\n",
    "        return output_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomT5Model(T5ForConditionalGeneration):\n",
    "    def __init__(self, tokenizer, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def generate(\n",
    "        self,\n",
    "        input_ids,\n",
    "        attention_mask=None,\n",
    "        block_topic=-1,\n",
    "        lda_model=None,\n",
    "        dictionary=None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        # If a topic should be blocked, prepare the block_token_ids\n",
    "        block_token_ids = []\n",
    "        if block_topic >= 0 and lda_model is not None and dictionary is not None:\n",
    "            top_words = get_top_topic_words(lda_model, block_topic)\n",
    "            block_token_ids = [\n",
    "                self.tokenizer.encode(word, add_special_tokens=False)[0]\n",
    "                for word in top_words\n",
    "            ]\n",
    "\n",
    "        # Custom logits manipulation\n",
    "        def custom_logits_processor(logits, input_ids):\n",
    "            if block_token_ids:\n",
    "                logits[:, block_token_ids] = -float(\"inf\")\n",
    "            return logits\n",
    "\n",
    "        # Override the standard logits_processor in the generation loop\n",
    "        original_logits_processor = self._get_logits_processor(**kwargs)\n",
    "\n",
    "        def combined_logits_processor(logits, input_ids):\n",
    "            logits = original_logits_processor(logits, input_ids)\n",
    "            return custom_logits_processor(logits, input_ids)\n",
    "\n",
    "        self._get_logits_processor = lambda **kwargs: combined_logits_processor\n",
    "\n",
    "        # Generate output sequences\n",
    "        output_sequences = super().generate(\n",
    "            input_ids=input_ids, attention_mask=attention_mask, **kwargs\n",
    "        )\n",
    "\n",
    "        # Reset logits processor to its original state\n",
    "        self._get_logits_processor = lambda **kwargs: original_logits_processor\n",
    "\n",
    "        return output_sequences\n",
    "\n",
    "\n",
    "# Usage remains the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "T5_base_tokenizer = T5Tokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "custom_model = CustomT5Model(\n",
    "    tokenizer=T5_base_tokenizer,\n",
    "    config=T5ForConditionalGeneration.from_pretrained(\"google/flan-t5-base\").config,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "GenerationMixin._get_logits_processor() got an unexpected keyword argument 'max_length'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Assuming 'inputs' is a dictionary with 'input_ids' and 'attention_mask'\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m custom_output_sequences \u001b[38;5;241m=\u001b[39m \u001b[43mcustom_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43minput_ids\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mattention_mask\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mblock_topic\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Topic to block\u001b[39;49;00m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlda_model\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlda\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Your LDA model\u001b[39;49;00m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdictionary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdictionary\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Your dictionary\u001b[39;49;00m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmin_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_beams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_return_sequences\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 31\u001b[0m, in \u001b[0;36mCustomT5Model.generate\u001b[0;34m(self, input_ids, attention_mask, block_topic, lda_model, dictionary, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m logits\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Override the standard logits_processor in the generation loop\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m original_logits_processor \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_logits_processor\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombined_logits_processor\u001b[39m(logits, input_ids):\n\u001b[1;32m     34\u001b[0m     logits \u001b[38;5;241m=\u001b[39m original_logits_processor(logits, input_ids)\n",
      "\u001b[0;31mTypeError\u001b[0m: GenerationMixin._get_logits_processor() got an unexpected keyword argument 'max_length'"
     ]
    }
   ],
   "source": [
    "# Assuming 'inputs' is a dictionary with 'input_ids' and 'attention_mask'\n",
    "custom_output_sequences = custom_model.generate(\n",
    "    input_ids=inputs[\"input_ids\"],\n",
    "    attention_mask=inputs[\"attention_mask\"],\n",
    "    block_topic=0,  # Topic to block\n",
    "    lda_model=lda,  # Your LDA model\n",
    "    dictionary=dictionary,  # Your dictionary\n",
    "    max_length=100,\n",
    "    min_length=30,\n",
    "    num_beams=1,\n",
    "    num_return_sequences=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Output from custom model:\\n\" + 100 * \"-\")\n",
    "for idx, output_sequence in enumerate(custom_output_sequences):\n",
    "    output = T5_base_tokenizer.decode(output_sequence, skip_special_tokens=True)\n",
    "    print(\"Input: {}\".format(sentences[idx]))\n",
    "    print(\"Generated summary {}: {}\".format(idx + 1, output))\n",
    "    print(100 * \"-\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_summaries = output_sequences.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(original_summaries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Output:\\n\" + 100 * \"-\")\n",
    "for idx, output_sequence in enumerate(original_summaries):\n",
    "    output = T5_base_tokenizer.decode(output_sequence, skip_special_tokens=True)\n",
    "    print(\"Input: {}\".format(sentences[idx]))\n",
    "    print(\"Generated summary {}: {}\".format(idx + 1, output))\n",
    "    print(100 * \"-\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP_proj_basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
